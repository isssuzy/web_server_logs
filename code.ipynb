from pyspark.sql import SparkSession
from pyspark.sql.functions import to_timestamp, col, count, sum, to_date

# создаем спрак сессию
spark = SparkSession.builder.appName('records').getOrCreate()

# чтения файла
data_df = spark.read.csv('web_server_logs.csv', header=True, inferSchema=True)

# переводим столбец timestamp в формат даты(формат timestamp не пригодился)
data_df = data_df.withColumn('timestamp', to_date('timestamp', 'yyyy-mm-dd' ))

# количество запросов для каждого IP, выводим 10 самых активных
top_ip = data_df.groupBy('ip').count().withColumnRenamed('count', 'request_count').orderBy(col('request_count').desc()).limit(10)
top_ip.show()

# количество запросов для каждого метода
methods = data_df.groupBy('method').count().withColumnRenamed("count","method_count").orderBy(col('method_count').desc())
methods.show()

# размер ответов по датам
num_of_404 = data_df.filter(col('response_code')==404).groupBy('timestamp').agg(sum(col('response_size')).alias('response_size')).orderBy('timestamp')
num_of_404.show()
